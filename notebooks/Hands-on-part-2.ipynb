{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHeOoEVlkXib"
   },
   "source": [
    "# Standard Practices for Data Processing and Multimodal Feature Extraction in Recommendation with DataRec and Ducho (D&D4Rec) (2nd Hands-On Session)\n",
    "\n",
    "‚≠ê **The 19th ACM Conference on Recommender Systems** ‚≠ê\n",
    "\n",
    "*Prague (Czech Republic), September 26th, 2025*\n",
    "\n",
    "<div>\n",
    "  <img src=\"http://github.com/sisinflab/DnD4Rec-tutorial/blob/main/images/DD4Rec-logo.jpg?raw=true\" alt=\"d&d4Rec\" width=\"108\">\n",
    "  <img src=\"https://recsys.acm.org/wp-content/uploads/2024/10/RecSys2025_website_header.jpg\" alt=\"SisInfLab\" width=\"600\">\n",
    "  <img src=\"https://recsys.acm.org/wp-content/uploads/2024/10/RecSys2025_logo_transparent.png\" alt=\"recsys\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "üßë Speaker: [Matteo Attimonelli](https://matteoattimonelli.github.io/)\n",
    "\n",
    "If you use this code for your experiments, please cite our recent work (on arXiv) üôè\n",
    "\n",
    "![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Ducho-meets-Elliot)\n",
    " [![arXiv](https://img.shields.io/badge/arXiv-2409.15857-b31b1b.svg)](https://arxiv.org/abs/2409.15857)\n",
    "\n",
    " <img src=\"https://github.com/sisinflab/Ducho-meets-Elliot/blob/master/framework.png?raw=true\"  width=\"700\">\n",
    "\n",
    "```\n",
    "@article{DBLP:journals/corr/abs-2409-15857,\n",
    "  author       = {Matteo Attimonelli and\n",
    "                  Danilo Danese and\n",
    "                  Angela Di Fazio and\n",
    "                  Daniele Malitesta and\n",
    "                  Claudio Pomo and\n",
    "                  Tommaso Di Noia},\n",
    "  title        = {Ducho meets Elliot: Large-scale Benchmarks for Multimodal Recommendation},\n",
    "  journal      = {CoRR},\n",
    "  volume       = {abs/2409.15857},\n",
    "  year         = {2024}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXW5FkgZjC65"
   },
   "source": [
    "## Clone the repository\n",
    "\n",
    "First, we clone the repository to exploit the Ducho + Elliot experimental environment üêë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akRmwGzZi0vQ",
    "outputId": "64b58104-38dd-452c-e759-2e20b6951125"
   },
   "outputs": [],
   "source": [
    "%cd /Users/matteoattimonelli/Desktop/Tutorial\n",
    "!git clone --recursive https://github.com/sisinflab/Ducho-meets-Elliot.git\n",
    "%cd Ducho-meets-Elliot/\n",
    "!git pull --recurse-submodules\n",
    "!git submodule update --remote --merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heMk9rBDvVqk"
   },
   "source": [
    "## Set up the working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAltQI_tmJXs"
   },
   "source": [
    "Now, we setup the proper environment to run the experiments. We provide a proper file with all the dependencies to facilitate the environment creation! üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9bPQ1DRuFUV",
    "outputId": "6860a4ef-4a54-457e-ce8e-1ccc70595088"
   },
   "outputs": [],
   "source": [
    "!conda env create -f ducho_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvxiGp7LvZPs"
   },
   "source": [
    "## Download and visualize the multimodal recommendation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OPlAe10vi4E"
   },
   "source": [
    "We're now set to download the multimodal recommendation dataset. For the sake of this lecture, we consider the popular **[Amazon Product Reviews](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews)** dataset üõí\n",
    "\n",
    "Specifically, we consider the following product categories:\n",
    "\n",
    "| **Datasets**     | **# Users** | **# Items** | **# Interactions** | **Sparsity (%)** |\n",
    "|------------------|-------------|-------------|--------------------|------------------|\n",
    "| Office Products  | 4,471       | 1,703       | 20,608             | 99.73%           |\n",
    "| Digital Music    | 5,082       | 2,338       | 30,623             | 99.74%           |\n",
    "| Baby             | 19,100      | 6,283       | 80,931             | 99.93%           |\n",
    "| Toys & Games     | 19,241      | 11,101      | 89,558             | 99.96%           |\n",
    "| Beauty           | 21,752      | 11,145      | 100,834            | 99.96%           |\n",
    "\n",
    "For the sake of this hands-on session, we will consider only the \"Office Products\" dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL ONLY IF YOU HAVE TIME TO WASTE :-)\n",
    "\n",
    "%cd Ducho\n",
    "!mkdir -p ./local/data/demo_office\n",
    "%cp ../../reviews_Office_Products_5.tsv ./local/data/demo_office\n",
    "\n",
    "# data taken from: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html\n",
    "import os.path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_image(url, image, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 429:\n",
    "        print('Too many requests, waiting...')\n",
    "        time.sleep(60)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return image\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path,'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "def download_dataset(file_name, save_path):\n",
    "    base_url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/'\n",
    "    response = requests.get(f'{base_url}{file_name}', stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(f'{save_path}/{file_name}', 'wb') as file:\n",
    "            bar = tqdm(total=int(response.headers.get('content-length')))\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "\n",
    "\n",
    "folder = './local/data/demo_office'\n",
    "name = 'Office_Products'\n",
    "\n",
    "core = 5\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "file_name = f'meta_{name}.json.gz'\n",
    "if not os.path.exists(f'{folder}/{file_name}'):\n",
    "    download_dataset(file_name, folder)\n",
    "\n",
    "reviews = pd.read_csv(f'{folder}/reviews_{name}_{core}.tsv', sep='\\t', names=['reviewerID', 'asin', 'overall'])\n",
    "\n",
    "# check reviewerID, asin, and overall are not nan and remove duplicates\n",
    "reviews = reviews[reviews['reviewerID'].notna()]\n",
    "reviews = reviews[reviews['asin'].notna()]\n",
    "reviews = reviews[reviews['overall'].notna()]\n",
    "reviews = reviews.drop_duplicates()\n",
    "\n",
    "meta = getDF(f'{folder}/meta_{name}.json.gz')[['asin', 'description', 'imUrl']]\n",
    "\n",
    "# check asin, description, and imUrl are not nan\n",
    "meta = meta[meta['asin'].notna()]\n",
    "meta = meta[meta['description'].notna()]\n",
    "meta = meta[meta['description'].str.len() > 0]\n",
    "meta = meta[meta['imUrl'].notna()]\n",
    "meta = meta[meta['imUrl'].str.len() > 0]\n",
    "meta = meta.drop_duplicates()\n",
    "\n",
    "# merge meta and reviews for 5-core\n",
    "meta_reviews = pd.merge(reviews, meta, how='inner', on='asin')\n",
    "meta_reviews.drop_duplicates(inplace=True)\n",
    "meta = meta_reviews[['asin', 'description', 'imUrl']].drop_duplicates()\n",
    "reviews = meta_reviews[['reviewerID', 'asin', 'overall']]\n",
    "\n",
    "# get unavailable items\n",
    "all_items = set(meta['asin'].tolist())\n",
    "items_nan_description = set(meta[meta['description'].isna()]['asin'].tolist()).union(set(\n",
    "    meta[meta['description'].notna()][\n",
    "        meta[meta['description'].notna()]['description'].str.contains('nan|Nan|NaN|naN|n\\/a|N\\/A', regex=True)][\n",
    "        'asin'].tolist()))\n",
    "items_nan_url = set(meta[meta['imUrl'].isna()]['asin'].tolist()).union(set(meta[meta['imUrl'].notna()][\n",
    "                                                                            meta[meta['imUrl'].notna()][\n",
    "                                                                                'imUrl'].str.contains(\n",
    "                                                                                'nan|Nan|NaN|naN|n\\/a|N\\/A',\n",
    "                                                                                regex=True)]['asin'].tolist()))\n",
    "items_empty_description = set(\n",
    "    meta[meta['description'].notna()][meta[meta['description'].notna()]['description'].str.len() == 0][\n",
    "        'asin'].tolist())\n",
    "items_empty_url = set(\n",
    "    meta[meta['imUrl'].notna()][meta[meta['imUrl'].notna()]['imUrl'].str.len() == 0]['asin'].tolist())\n",
    "remaining_items = all_items.difference(items_nan_description).difference(items_nan_url).difference(\n",
    "    items_empty_description).difference(items_empty_url)\n",
    "print(f'All items: {len(all_items)}')\n",
    "print(f'All users: {reviews[\"reviewerID\"].nunique()}')\n",
    "print(f'All interactions: {len(reviews)}')\n",
    "print(f'Nan description: {len(items_nan_description)}')\n",
    "print(f'Nan url: {len(items_nan_url)}')\n",
    "print(f'Empty description: {len(items_empty_description)}')\n",
    "print(f'Empty url: {len(items_empty_url)}')\n",
    "\n",
    "missing_visual = items_nan_url.union(items_empty_url)\n",
    "missing_textual = items_nan_description.union(items_empty_description)\n",
    "\n",
    "# save original df\n",
    "meta.to_csv(f'{folder}/original_meta.tsv', sep='\\t', index=None)\n",
    "reviews.to_csv(f'{folder}/original_reviews.tsv', sep='\\t', index=None, header=None)\n",
    "\n",
    "meta = meta[meta['asin'].isin(all_items.difference(missing_textual))]\n",
    "reviews = reviews[reviews['asin'].isin(remaining_items)]\n",
    "\n",
    "print(len(meta[meta['description'].isna()]))\n",
    "print(len(meta[meta['description'].str.len() == 0]))\n",
    "print(len(meta[meta['description'].str.contains('nan')]))\n",
    "print(len(meta[meta['description'].str.contains('Nan')]))\n",
    "print(len(meta[meta['description'].str.contains('NaN')]))\n",
    "print(len(meta[meta['description'].str.contains('n\\/a')]))\n",
    "print(len(meta[meta['description'].str.contains('N\\/A')]))\n",
    "\n",
    "print(f'Remaining users: {reviews[\"reviewerID\"].nunique()}')\n",
    "print(f'Remaining interactions: {len(reviews)}')\n",
    "\n",
    "if not os.path.exists(f'{folder}/images/'):\n",
    "    os.makedirs(f'{folder}/images/')\n",
    "\n",
    "images = []\n",
    "with tqdm(total=len(meta)) as t:\n",
    "    for index, row in meta.iterrows():\n",
    "        if pd.notna(row['imUrl']):\n",
    "            output = download_image(row['imUrl'], row['asin'], f'{folder}/images/{row[\"asin\"]}.jpg')\n",
    "            if output is None:\n",
    "                missing_visual.add(row['asin'])\n",
    "            images.append(output)\n",
    "            t.update()\n",
    "\n",
    "broken_urls = set([im for im in images if im is None])\n",
    "print(f'Broken url: {len(broken_urls)}')\n",
    "remaining_items = remaining_items.intersection(set([im for im in images if im]))\n",
    "print(f'Remaining items: {len(remaining_items)}')\n",
    "\n",
    "if len(broken_urls) > 0:\n",
    "    meta = meta[~meta['asin'].isin(missing_visual)]\n",
    "    reviews = reviews[~reviews['asin'].isin(missing_visual)]\n",
    "\n",
    "meta.to_csv(f'{folder}/meta.tsv', sep='\\t', index=None)\n",
    "reviews.to_csv(f'{folder}/reviews.tsv', sep='\\t', index=None, header=None)\n",
    "\n",
    "# save missing items\n",
    "pd.DataFrame(list(missing_visual)).to_csv(f'{folder}/missing_visual.tsv', sep='\\t',\n",
    "                                        index=None, header=None)\n",
    "pd.DataFrame(list(missing_textual)).to_csv(f'{folder}/missing_textual.tsv', sep='\\t',\n",
    "                                        index=None,header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHERWISE, HERE'S AN ALREADY-PREPARED VERSION OF THE DATASET\n",
    "\n",
    "import gdown\n",
    "\n",
    "%cd Ducho\n",
    "!mkdir -p local/data\n",
    "gdown.download(f'https://drive.google.com/uc?id=1shg7SJJqTLGzjyBC8z-KAnRhcK7cRGEp', 'demo_office.zip', quiet=False)\n",
    "!mv demo_office.zip local/data/\n",
    "!unzip local/data/demo_office.zip -d local/data/\n",
    "%rm local/data/demo_office.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A348xeBhBTFp"
   },
   "source": [
    "We can visualize one random item from the dataset üëì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_QkRwkpG54Vk",
    "outputId": "8c5b7124-0267-438d-9ab1-5bf79c840136"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "meta = pd.read_csv('local/data/demo_office/meta.tsv', sep='\\t')\n",
    "random_item = random.choice(meta['asin'].tolist())\n",
    "description = meta[meta[\"asin\"]==random_item][\"description\"].values[0]\n",
    "display(HTML(f\"ASIN: {random_item}\\n<div style='white-space: pre-wrap; width: 100%;'>{description}</div>\"))\n",
    "img = Image.open(f'local/data/demo_office/images/{random_item}.jpg')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = [\"USER\", \"Rating\"]\n",
    "reviews = pd.read_csv('local/data/demo_office/reviews.tsv', sep='\\t', header=None)\n",
    "current_reviews = reviews[reviews[1]==random_item]\n",
    "\n",
    "for idx, row in current_reviews.iterrows():\n",
    "  table.add_row([row[0], row[2]])\n",
    "\n",
    "display(HTML('Clicked by:'))\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJTKjUaZHodP"
   },
   "source": [
    "## Check if GPU is available\n",
    "\n",
    "Before running any GPU-bound process, let's check if the GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5USH4HhKHrIB",
    "outputId": "43abfde2-e24a-4817-d869-2abd3611676d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeAhxZ6DgHxp"
   },
   "source": [
    "## Extract multimodal features with Ducho\n",
    "\n",
    "![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Ducho)\n",
    " [![arXiv](https://img.shields.io/badge/arXiv-2403.04503-b31b1b.svg)](https://arxiv.org/abs/2403.04503)\n",
    "\n",
    " If you use Ducho for your experiments, please cite our papers üôè\n",
    "\n",
    "<div>\n",
    "  <img src=\"https://github.com/sisinflab/Ducho/raw/main/docs/source/img/ducho_v2_overview.png\" alt=\"duccio\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "```\n",
    "@inproceedings{DBLP:conf/www/AttimonelliDMPG24,\n",
    "  author       = {Matteo Attimonelli and\n",
    "                  Danilo Danese and\n",
    "                  Daniele Malitesta and\n",
    "                  Claudio Pomo and\n",
    "                  Giuseppe Gassi and\n",
    "                  Tommaso Di Noia},\n",
    "  title        = {Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction\n",
    "                  of Multimodal Features in Recommendation},\n",
    "  booktitle    = {{WWW} (Companion Volume)},\n",
    "  pages        = {1075--1078},\n",
    "  publisher    = {{ACM}},\n",
    "  year         = {2024}\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "@inproceedings{DBLP:conf/mm/MalitestaGPN23,\n",
    "  author       = {Daniele Malitesta and\n",
    "                  Giuseppe Gassi and\n",
    "                  Claudio Pomo and\n",
    "                  Tommaso Di Noia},\n",
    "  title        = {Ducho: {A} Unified Framework for the Extraction of Multimodal Features\n",
    "                  in Recommendation},\n",
    "  booktitle    = {{ACM} Multimedia},\n",
    "  pages        = {9668--9671},\n",
    "  publisher    = {{ACM}},\n",
    "  year         = {2023}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynLIbsKwm_Bh"
   },
   "source": [
    "Now, we are all set to extract multimodal product features through Ducho ü¶æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE CONFIGURATION FILE FOR DUCHO\n",
    "def create_config():\n",
    "  import yaml\n",
    "\n",
    "  config_ducho = \"\"\"dataset_path: ./local/data/demo_office\n",
    "gpu list: 0\n",
    "visual:\n",
    "    items:\n",
    "        input_path: images\n",
    "        output_path: visual_embeddings_32\n",
    "        model: [\n",
    "            { model_name: ResNet50,\n",
    "              output_layers: avgpool,\n",
    "              reshape: [224, 224],\n",
    "              preprocessing: zscore,\n",
    "              backend: torch,\n",
    "              batch_size: 32\n",
    "            }\n",
    "        ]\n",
    "\n",
    "textual:\n",
    "    items:\n",
    "        input_path: meta.tsv\n",
    "        item_column: asin\n",
    "        text_column: description\n",
    "        output_path: textual_embeddings_32\n",
    "        model: [\n",
    "          { model_name: sentence-transformers/all-mpnet-base-v2,\n",
    "              output_layers: 1,\n",
    "              clear_text: False,\n",
    "              backend: sentence_transformers,\n",
    "              batch_size: 32\n",
    "          }\n",
    "        ]\n",
    "\n",
    "visual_textual:\n",
    "    items:\n",
    "        input_path: {visual: images, textual: meta.tsv}\n",
    "        item_column: asin\n",
    "        text_column: description\n",
    "        output_path: {visual: visual_embeddings_32, textual: textual_embeddings_32}\n",
    "        model: [\n",
    "          { model_name: openai/clip-vit-base-patch16,\n",
    "              backend: transformers,\n",
    "              output_layers: 1,\n",
    "              batch_size: 32\n",
    "          }\n",
    "        ]\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  ducho_dir = f\"demos/demo_office/config.yml\"\n",
    "  with open(ducho_dir, 'w') as conf_file:\n",
    "      conf_file.write(config_ducho)\n",
    "\n",
    "# RUN THE EXTRACTION WITH DUCHO\n",
    "\n",
    "from ducho.runner.Runner import MultimodalFeatureExtractor\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n",
    "       When seed is None, disables deterministic mode.\n",
    "    :param seed: an integer to your choosing\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":16:8\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed()\n",
    "    extractor_obj = MultimodalFeatureExtractor(config_file_path='./demos/demo_office/config.yml')\n",
    "    extractor_obj.execute_extractions()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_config()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjZn0kK1jVr3"
   },
   "source": [
    "## Dataset splitting and features mapping\n",
    "\n",
    "![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Formal-Multimod-Rec)\n",
    " [![arXiv](https://img.shields.io/badge/arXiv-2309.05273-b31b1b.svg)](https://arxiv.org/abs/2309.05273)\n",
    "\n",
    "If everything went smoothly with the features extraction, now we can: (i) split the original dataset into train/validation/test set ‚úÇ (ii) map the multimodal item features to ids aligned with the training set üóæ\n",
    "\n",
    "To this end, we will use Elliot, our framework for rigorous and reproducibile recommender systems evaluation.\n",
    "\n",
    "If you find it useful for your research, please cite our works üôè\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "@article{10.1145/3662738,\n",
    "author = {Malitesta, Daniele and Cornacchia, Giandomenico and Pomo, Claudio and Merra, Felice Antonio and Di Noia, Tommaso and Di Sciascio, Eugenio},\n",
    "title = {Formalizing Multimedia Recommendation through Multimodal Deep Learning},\n",
    "year = {2024},\n",
    "publisher = {Association for Computing Machinery},\n",
    "address = {New York, NY, USA},\n",
    "url = {https://doi.org/10.1145/3662738},\n",
    "doi = {10.1145/3662738},\n",
    "note = {Just Accepted},\n",
    "journal = {ACM Trans. Recomm. Syst.},\n",
    "month = {apr},\n",
    "keywords = {Multimodal Deep Learning, Multimedia Recommender Systems, Benchmarking}\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "@inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n",
    "  author       = {Vito Walter Anelli and\n",
    "                  Alejandro Bellog{\\'{\\i}}n and\n",
    "                  Antonio Ferrara and\n",
    "                  Daniele Malitesta and\n",
    "                  Felice Antonio Merra and\n",
    "                  Claudio Pomo and\n",
    "                  Francesco Maria Donini and\n",
    "                  Tommaso Di Noia},\n",
    "  title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n",
    "                  Recommender Systems Evaluation},\n",
    "  booktitle    = {{SIGIR}},\n",
    "  pages        = {2405--2414},\n",
    "  publisher    = {{ACM}},\n",
    "  year         = {2021}\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Dataset splitting**\n",
    "\n",
    "* Train = 80% dataset\n",
    "\n",
    "* Test = 20% dataset\n",
    "\n",
    "* Valid = 10% train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "!git clone https://github.com/sisinflab/DataRec.git\n",
    "%cd DataRec\n",
    "\n",
    "from datarec.io import read_tabular\n",
    "from datarec.data.dataset import DataRec\n",
    "from datarec.splitters import RandomHoldOut\n",
    "from datarec.io import write_tabular\n",
    "\n",
    " \n",
    "path=\"../Ducho/local/data/demo_office/reviews.tsv\"\n",
    "raw = read_tabular(\n",
    "    filepath=path,\n",
    "    sep=\"\\t\",\n",
    "    user_col=0,\n",
    "    item_col=1,\n",
    "    rating_col=2,\n",
    "    header=None\n",
    ")\n",
    " \n",
    "data = DataRec(rawdata=raw, dataset_name=\"AmazonOffice\", version_name=\"ducho\")\n",
    "  \n",
    "splitter = RandomHoldOut(test_ratio=0.2, val_ratio=0.1, seed=42)\n",
    "split_result = splitter.run(data)\n",
    " \n",
    "train_data = split_result['train']\n",
    "val_data = split_result['val']\n",
    "test_data = split_result['test']\n",
    " \n",
    "\n",
    "file_path = '../data/office'\n",
    "write_tabular(train_data.to_rawdata(), path=f\"{file_path}/train.tsv\", sep='\\t', header=False, timestamp=False) \n",
    "write_tabular(val_data.to_rawdata(), path=f\"{file_path}/val.tsv\", sep='\\t', header=False, timestamp=False) \n",
    "write_tabular(test_data.to_rawdata(), path=f\"{file_path}/test.tsv\", sep='\\t', header=False, timestamp=False)  \n",
    "\n",
    "%cd /Users/matteoattimonelli/Desktop/Tutorial/Ducho-meets-Elliot\n",
    "\n",
    "%mv ./Ducho/local/data/demo_office/visual_embeddings_32 ./data/office\n",
    "%mv ./Ducho/local/data/demo_office/textual_embeddings_32 ./data/office\n",
    "\n",
    "%cd ./data/office\n",
    "\n",
    "!pwd\n",
    "\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "# MAP ITEMS TO NUMERICAL IDS\n",
    "train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "val = pd.read_csv('val.tsv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "\n",
    "df = pd.concat([train, val, test], axis=0)\n",
    "\n",
    "users = df[0].unique()\n",
    "items = df[1].unique()\n",
    "\n",
    "users_map = {u: idx for idx, u in enumerate(users)}\n",
    "items_map = {i: idx for idx, i in enumerate(items)}\n",
    "\n",
    "train[0] = train[0].map(users_map)\n",
    "train[1] = train[1].map(items_map)\n",
    "\n",
    "val[0] = val[0].map(users_map)\n",
    "val[1] = val[1].map(items_map)\n",
    "\n",
    "test[0] = test[0].map(users_map)\n",
    "test[1] = test[1].map(items_map)\n",
    "\n",
    "train.to_csv('train_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "val.to_csv('val_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "test.to_csv('test_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "\n",
    "visual_embeddings_folder = f'visual_embeddings_32/torch/ResNet50/avgpool'\n",
    "textual_embeddings_folder = f'textual_embeddings_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n",
    "\n",
    "visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/torch/ResNet50/avgpool'\n",
    "textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n",
    "\n",
    "if not os.path.exists(visual_embeddings_folder_indexed):\n",
    "    os.makedirs(visual_embeddings_folder_indexed)\n",
    "\n",
    "if not os.path.exists(textual_embeddings_folder_indexed):\n",
    "    os.makedirs(textual_embeddings_folder_indexed)\n",
    "\n",
    "for key, value in items_map.items():\n",
    "    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n",
    "    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n",
    "\n",
    "\n",
    "visual_embeddings_folder = f'visual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "textual_embeddings_folder = f'textual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "\n",
    "visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "\n",
    "if not os.path.exists(visual_embeddings_folder_indexed):\n",
    "    os.makedirs(visual_embeddings_folder_indexed)\n",
    "\n",
    "if not os.path.exists(textual_embeddings_folder_indexed):\n",
    "    os.makedirs(textual_embeddings_folder_indexed)\n",
    "\n",
    "for key, value in items_map.items():\n",
    "    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n",
    "    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n",
    "\n",
    "\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZKTpmUmi5pu"
   },
   "outputs": [],
   "source": [
    "#### OR IF YOU WANT TO PERFORM THE SPLITTING WITH ELLIOT :-)\n",
    "\n",
    "%cd /Users/matteoattimonelli/Desktop/Tutorial/Ducho-meets-Elliot\n",
    "\n",
    "%mv ./Ducho/local/data/demo_office/visual_embeddings_32 ./data/office\n",
    "%mv ./Ducho/local/data/demo_office/textual_embeddings_32 ./data/office\n",
    "\n",
    "%cp ./Ducho/local/data/demo_office/reviews.tsv ./data/office\n",
    "\n",
    "split_config = '''\n",
    "experiment:\n",
    "  backend: pytorch\n",
    "  data_config:\n",
    "    strategy: dataset\n",
    "    dataset_path: ../data/office/reviews.tsv\n",
    "  splitting:\n",
    "    save_on_disk: True\n",
    "    save_folder: ../data/office_splits/\n",
    "    test_splitting:\n",
    "      strategy: random_subsampling\n",
    "      test_ratio: 0.2\n",
    "    validation_splitting:\n",
    "      strategy: random_subsampling\n",
    "      test_ratio: 0.1\n",
    "  dataset: office\n",
    "  top_k: 20\n",
    "  evaluation:\n",
    "    cutoffs: [ 10, 20 ]\n",
    "    simple_metrics: [ Recall, nDCG ]\n",
    "  gpu: 0\n",
    "  external_models_path: ../external/models/__init__.py\n",
    "  models:\n",
    "    MostPop:\n",
    "      meta:\n",
    "        verbose: True\n",
    "        save_recs: False\n",
    "'''\n",
    "\n",
    "split_dir = f\"./config_files/split_office.yml\"\n",
    "with open(split_dir, 'w') as conf_file:\n",
    "    conf_file.write(split_config)\n",
    "\n",
    "# SPLIT INTO TRAIN/VAL/TEST\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "!python3 run_split.py --dataset office\n",
    "\n",
    "%cd ./data/office\n",
    "\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "# MAP ITEMS TO NUMERICAL IDS\n",
    "train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "val = pd.read_csv('val.tsv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "\n",
    "df = pd.concat([train, val, test], axis=0)\n",
    "\n",
    "users = df[0].unique()\n",
    "items = df[1].unique()\n",
    "\n",
    "users_map = {u: idx for idx, u in enumerate(users)}\n",
    "items_map = {i: idx for idx, i in enumerate(items)}\n",
    "\n",
    "train[0] = train[0].map(users_map)\n",
    "train[1] = train[1].map(items_map)\n",
    "\n",
    "val[0] = val[0].map(users_map)\n",
    "val[1] = val[1].map(items_map)\n",
    "\n",
    "test[0] = test[0].map(users_map)\n",
    "test[1] = test[1].map(items_map)\n",
    "\n",
    "train.to_csv('train_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "val.to_csv('val_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "test.to_csv('test_indexed.tsv', sep='\\t', index=False, header=None)\n",
    "\n",
    "visual_embeddings_folder = f'visual_embeddings_32/torch/ResNet50/avgpool'\n",
    "textual_embeddings_folder = f'textual_embeddings_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n",
    "\n",
    "visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/torch/ResNet50/avgpool'\n",
    "textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n",
    "\n",
    "if not os.path.exists(visual_embeddings_folder_indexed):\n",
    "    os.makedirs(visual_embeddings_folder_indexed)\n",
    "\n",
    "if not os.path.exists(textual_embeddings_folder_indexed):\n",
    "    os.makedirs(textual_embeddings_folder_indexed)\n",
    "\n",
    "for key, value in items_map.items():\n",
    "    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n",
    "    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n",
    "\n",
    "\n",
    "visual_embeddings_folder = f'visual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "textual_embeddings_folder = f'textual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "\n",
    "visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "\n",
    "if not os.path.exists(visual_embeddings_folder_indexed):\n",
    "    os.makedirs(visual_embeddings_folder_indexed)\n",
    "\n",
    "if not os.path.exists(textual_embeddings_folder_indexed):\n",
    "    os.makedirs(textual_embeddings_folder_indexed)\n",
    "\n",
    "for key, value in items_map.items():\n",
    "    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n",
    "    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n",
    "\n",
    "\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqdHyO8inLnw"
   },
   "source": [
    "The downloaded multimodal dataset has the following structure:\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ office\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ visual_embeddings_indexed_32\n",
    "|       ‚îú‚îÄ‚îÄ torch\n",
    "|          ‚îú‚îÄ‚îÄ ResNet50\n",
    "|             ‚îú‚îÄ‚îÄ avgpool\n",
    "‚îÇ                ‚îú‚îÄ‚îÄ 0.npy\n",
    "‚îÇ                ‚îú‚îÄ‚îÄ 1.npy\n",
    "‚îÇ                ‚îú‚îÄ‚îÄ ...\n",
    "|       ‚îú‚îÄ‚îÄ transformers\n",
    "|          ‚îú‚îÄ‚îÄ openai\n",
    "|             ‚îú‚îÄ‚îÄ clip-vit-base-patch16\n",
    "|                ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ ...\n",
    "|\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ textual_embeddings_indexed_32\n",
    "|       ‚îú‚îÄ‚îÄ sentence_transformers\n",
    "|          ‚îú‚îÄ‚îÄ sentence-transformers\n",
    "|             ‚îú‚îÄ‚îÄ all-mpnet-base-v2\n",
    "|                ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ ...\n",
    "|       ‚îú‚îÄ‚îÄ transformers\n",
    "|          ‚îú‚îÄ‚îÄ openai\n",
    "|             ‚îú‚îÄ‚îÄ clip-vit-base-patch16\n",
    "|                ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n",
    "‚îÇ                   ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train_indexed.tsv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ val_indexed.tsv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_indexed.tsv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z02UP4r9cVEd"
   },
   "source": [
    "## Configure and run the experiments\n",
    "Let's set the hyper-parameters for the model to be trained and tested. We will focus on VBPR in a modified version which adopts multimodal features. We train and evaluate it on Amazon Office."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61kNdayAckeH"
   },
   "source": [
    "### First multimodal features configuration\n",
    "\n",
    "We start with the configuration ResNet50 (visual) + SentenceBert (textual), the most common one from the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxBi4sqkccQW"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_filename = 'hands-on_resnet50_sentencebert'\n",
    "elliot_config_1 = {\n",
    "  'experiment': {\n",
    "    'backend': 'pytorch',\n",
    "    'data_config': {\n",
    "      'strategy': 'fixed',\n",
    "      'train_path': '../data/{0}/train_indexed.tsv',\n",
    "      'validation_path': '../data/{0}/val_indexed.tsv',\n",
    "      'test_path': '../data/{0}/test_indexed.tsv',\n",
    "      'side_information': [\n",
    "        {\n",
    "            'dataloader': 'VisualAttribute',\n",
    "            'visual_features': '../data/{0}/visual_embeddings_indexed_32/torch/ResNet50/avgpool'\n",
    "        },\n",
    "        {\n",
    "            'dataloader': 'TextualAttribute',\n",
    "            'textual_features': '../data/{0}/textual_embeddings_indexed_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    'dataset': 'office',\n",
    "    'top_k': 20,\n",
    "    'evaluation': {\n",
    "      'cutoffs': [20],\n",
    "      'simple_metrics': ['Recall', 'Precision', 'nDCG', 'HR']\n",
    "    },\n",
    "    'gpu': 0,\n",
    "    'external_models_path': '../external/models/__init__.py',\n",
    "    'models': {\n",
    "      'external.VBPR': {\n",
    "        'meta': {\n",
    "          'hyper_opt_alg': 'grid',\n",
    "          'verbose': True,\n",
    "          'save_weights': False,\n",
    "          'save_recs': False,\n",
    "          'validation_rate': 10,\n",
    "          'validation_metric': 'Recall@20',\n",
    "          'restore': False\n",
    "        },\n",
    "        'epochs': 200,\n",
    "        'batch_size': 1024,\n",
    "        'factors': 64,\n",
    "        'lr': 0.005,\n",
    "        'l_w': 1e-5,\n",
    "        'n_layers': 1,\n",
    "        'comb_mod': 'concat',\n",
    "        'modalities': \"('visual','textual')\",\n",
    "        'loaders': \"('VisualAttribute','TextualAttribute')\",\n",
    "        'seed': 123\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(f'config_files/{config_filename}.yml', 'w') as file:\n",
    "    documents = yaml.dump(elliot_config_1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWN7Ns1re0Vf"
   },
   "source": [
    "### Run Elliot\n",
    "\n",
    "Now we are all set to run an experiment with VBPR on Amazon Office with the first multimodal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8igy2uce6ad"
   },
   "outputs": [],
   "source": [
    "from elliot.run import run_experiment\n",
    "\n",
    "\n",
    "run_experiment(f\"config_files/hands-on_resnet50_sentencebert.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM_lprmkfH9H"
   },
   "source": [
    "### Second multimodal features configuration\n",
    "\n",
    "Second, we prepare and run the second configuration for the multimodal feature extractors. In this case, we use CLIP, a popular multimodal model in the deep learning literature, but largely overlooked in the recommendation community.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RydIwe78ffpW"
   },
   "outputs": [],
   "source": [
    "%cd /Users/matteoattimonelli/Desktop/Tutorial/Ducho-meets-Elliot\n",
    "import yaml\n",
    "config_filename = 'hands-on_clip'\n",
    "elliot_config_2 = {\n",
    "  'experiment': {\n",
    "    'backend': 'pytorch',\n",
    "    'data_config': {\n",
    "      'strategy': 'fixed',\n",
    "      'train_path': '../data/{0}/train_indexed.tsv',\n",
    "      'validation_path': '../data/{0}/val_indexed.tsv',\n",
    "      'test_path': '../data/{0}/test_indexed.tsv',\n",
    "      'side_information': [\n",
    "        {\n",
    "            'dataloader': 'VisualAttribute',\n",
    "            'visual_features': '../data/{0}/visual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "        },\n",
    "        {\n",
    "            'dataloader': 'TextualAttribute',\n",
    "            'textual_features': '../data/{0}/textual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    'dataset': 'office',\n",
    "    'top_k': 20,\n",
    "    'evaluation': {\n",
    "      'cutoffs': [20],\n",
    "      'simple_metrics': ['Recall', 'Precision', 'nDCG', 'HR']\n",
    "    },\n",
    "    'gpu': 0,\n",
    "    'external_models_path': '../external/models/__init__.py',\n",
    "    'models': {\n",
    "      'external.VBPR': {\n",
    "        'meta': {\n",
    "          'hyper_opt_alg': 'grid',\n",
    "          'verbose': True,\n",
    "          'save_weights': False,\n",
    "          'save_recs': False,\n",
    "          'validation_rate': 10,\n",
    "          'validation_metric': 'Recall@20',\n",
    "          'restore': False\n",
    "        },\n",
    "        'epochs': 200,\n",
    "        'batch_size': 1024,\n",
    "        'factors': 64,\n",
    "        'lr': 0.005,\n",
    "        'l_w': 1e-5,\n",
    "        'n_layers': 1,\n",
    "        'comb_mod': 'concat',\n",
    "        'modalities': \"('visual','textual')\",\n",
    "        'loaders': \"('VisualAttribute','TextualAttribute')\",\n",
    "        'seed': 123\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(f'config_files/{config_filename}.yml', 'w') as file:\n",
    "    documents = yaml.dump(elliot_config_2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeP3QSryfm1B"
   },
   "source": [
    "### Run Elliot\n",
    "\n",
    "Now we are all set to run an experiment with VBPR on Amazon Office with the second multimodal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_osRKwlfqHo"
   },
   "outputs": [],
   "source": [
    "from elliot.run import run_experiment\n",
    "\n",
    "run_experiment(f\"config_files/hands-on_clip.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments\n",
    "\n",
    "We see that with different multimodal feature extractors, results are not the same. Indeed, with CLIP, we find (in most cases) improved recommendation performance than the usual ones obtained with ResNet50 + SentenceBert. That happens even without having explored a wide hyper-parameter space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# First configuration (ResNet50 + SentenceBert)\n",
    "\n",
    "{20: {'Recall': 0.08349616427404198, 'Precision': 0.009457434052757795, 'nDCG': 0.039479006837110385, 'HR': 0.1591726618705036}}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Second configuration (CLIP)\n",
    "\n",
    "{20: {'Recall': 0.0843764665257471, 'Precision': 0.009637290167865707, 'nDCG': 0.039173725010405266, 'HR': 0.16007194244604317}}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ducho)",
   "language": "python",
   "name": "ducho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
